{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ba1f9b42-c61b-4e30-8443-1bd126d2351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nbimporter\n",
    "from ColumnTransformers import *\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import json \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from catboost import Pool \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eebb8187-2fee-4cc0-ad8a-5826ab8b2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XgboostSearch(scale_pos_weight,df_X,y,metric=\"roc_auc\"): \n",
    "    param_grid_xgb = {\n",
    "        'n_estimators': [100],  \n",
    "        'learning_rate': [0.001],  \n",
    "        'max_depth': [3], \n",
    "        'subsample': [1.0],  \n",
    "        'colsample_bytree': [1.0],  \n",
    "        'gamma': [0.1], \n",
    "        'reg_lambda': [4], \n",
    "        'scale_pos_weight':[scale_pos_weight], # weight = 1 if negstive else wegiht=scale_pos_weight\n",
    "        }  \n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model=XGBClassifier()\n",
    "    grid_search_xgb = GridSearchCV(model, param_grid_xgb, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_xgb.fit(X,y)\n",
    "    best_params=grid_search_xgb.best_params_\n",
    "    with open(\"savedModels/XGBClassifier.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)    \n",
    "    return  best_params\n",
    "\n",
    "def LightgbmSearch(df_X,y,metric=\"roc_auc\"):\n",
    "    param_grid_lgb = {\n",
    "        'n_estimators': [100],\n",
    "        'learning_rate': [0.001],\n",
    "        'max_depth': [3],\n",
    "        'subsample': [0.7],\n",
    "        'colsample_bytree': [0.6],\n",
    "        'min_split_gain': [0],   # similar to gamma from XGB\n",
    "        'reg_lambda': [0], \n",
    "        'is_unbalance':[True] }\n",
    "\n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model=LGBMClassifier()\n",
    "    grid_search_lgb = GridSearchCV(model, param_grid_lgb, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_lgb.fit(X,y)\n",
    "    best_params=grid_search_lgb.best_params_\n",
    "    with open(\"savedModels/LGBMClassifier.json\", \"w\") as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    return  best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def AdaSearch(df_X,y,metric='roc_auc') : \n",
    "    param_grid_ada = {\n",
    "    'n_estimators': [100], \n",
    "    'learning_rate': [0.6],  # before amount of say\n",
    "    'estimator': [DecisionTreeClassifier(max_depth=2)#, \n",
    "                         # DecisionTreeClassifier(max_depth=3),\n",
    "                          #DecisionTreeClassifier(max_depth=5)\n",
    "                 ]\n",
    "    }\n",
    "    Lr=PipeLineGradient() \n",
    "    Lr.fit(df_X,y) \n",
    "    X=Lr.transform(df_X) \n",
    "    model= AdaBoostClassifier()\n",
    "    grid_search_ada = GridSearchCV(model, param_grid_ada, cv=5, scoring=metric, n_jobs=3)\n",
    "    grid_search_ada.fit(X,y)\n",
    "    best_params=grid_search_ada.best_params_\n",
    "    serializable_params = {k: str(v) for k, v in best_params.items()}\n",
    "    with open(\"savedModels/AdaBoost.json\", \"w\") as f:\n",
    "        json.dump(serializable_params, f, indent=4)\n",
    "    print(f\"Adaboost score: {123}\")\n",
    "    return best_params\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "eb7414ee-cbd5-4174-8b99-7e4d521520fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadCreateModel(modelName): \n",
    "    if modelName!=\"CatBoostClassifier\":\n",
    "        with open(f\"savedModels/{modelName}.json\", \"r\") as f:\n",
    "            best_params=json.load(f) \n",
    "    if modelName == \"AdaBoost\":\n",
    "            if \"learning_rate\" in best_params:\n",
    "                best_params[\"learning_rate\"] = float(best_params[\"learning_rate\"])\n",
    "            if \"n_estimators\" in best_params:\n",
    "                best_params[\"n_estimators\"] = int(best_params[\"n_estimators\"])\n",
    "            if \"estimator\" in best_params and isinstance(best_params[\"estimator\"], str):\n",
    "                estimator_str = best_params.pop(\"estimator\")\n",
    "                if estimator_str.startswith(\"DecisionTreeClassifier\"):\n",
    "                    estimator = eval(estimator_str) \n",
    "                else:\n",
    "                    raise ValueError(f\"Unsupported estimator: {estimator_str}\")\n",
    "                createdModel = AdaBoostClassifier(estimator=estimator)\n",
    "            else:\n",
    "                createdModel = AdaBoostClassifier()\n",
    "    elif modelName==\"XGBClassifier\":\n",
    "        createdModel=XGBClassifier(random_state=42, verbosity=0, n_jobs=-1)\n",
    "    elif modelName==\"LGBMClassifier\":\n",
    "        createdModel=LGBMClassifier(random_state=42 ,n_jobs=-1,verbosity=0)\n",
    "    else:   \n",
    "        with open(f\"savedModels/{modelName}.json\", \"r\") as f:\n",
    "            data=json.load(f) \n",
    "            best_params = data[\"params\"]\n",
    "        createdModel=CatBoostClassifier(random_state=42,verbose=0)\n",
    "    createdModel.set_params(**best_params) \n",
    "    return createdModel\n",
    "    \n",
    "def TestingModel(modelName,X_train,X_test,y_train,y_test,threshold=0.5 ): \n",
    "    \n",
    "    Model=ReadCreateModel(modelName) \n",
    "    if modelName!=\"CatBoostClassifier\":\n",
    "        Model=PipelineModel(Model)\n",
    "        Model.fit(X_train,y_train) \n",
    "        y_scores=Model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        X_train_transformed,cat_train=MergeNumCat(X_train)\n",
    "        X_test_transformed,cat_test=MergeNumCat(X_test)\n",
    "        pool_train=Pool(data=X_train_transformed,label=y_train,cat_features=cat_train)\n",
    "        pool_test = Pool(data=X_test_transformed, cat_features=cat_test)\n",
    "        Model.fit(pool_train)\n",
    "        y_scores= Model.predict_proba(pool_test)[:,1]\n",
    "        \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    aucPlot(fpr, tpr,roc_auc)\n",
    "\n",
    "    y_pred = (np.array(y_scores) > threshold).astype(int)\n",
    "    print(f\"AUC: {roc_auc}, F1 score: {f1_score(y_test,t_pred)}\")\n",
    "    return Model\n",
    "\n",
    "\n",
    "def aucPlot(fpr,tpr,roc_auc): \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\") \n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3c811393-fb6a-4c7c-91c6-ba223d703956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "28dd3937-a1d0-4795-987d-ac2d7f804713",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CatBoostTransformer(Numerical=['Transaction.Amount', 'Customer.Age','Account.Age.Days','Quantity']): \n",
    "    column_transformer = ColumnTransformer([\n",
    "        ('time_features', TimeTransformer(),[\"Transaction.Date\",\"Transaction.Hour\"]), \n",
    "        (\"high_amount\",HighAmountTransformer(),[\"Transaction.Amount\"]),\n",
    "        (\"numerical\",StandardScaler(),Numerical), \n",
    "        (\"age\",AgeTransfomer(),[\"Customer.Age\"]), \n",
    "        (\"dropColumns\",'drop',[\"Transaction.Date\",\"Transaction.Hour\"])],remainder=\"passthrough\") \n",
    "    return column_transformer\n",
    "\n",
    "def MergeNumCat(X):\n",
    "    CatTransformer=CatBoostTransformer() \n",
    "    CatTransformer.fit(X) \n",
    "    X_transformed=CatTransformer.transform(X)\n",
    "    categorical=[col for col in range(X_transformed.shape[1]) if len(np.unique(X_transformed[:, col]))<6] \n",
    "    X_transformed[:, categorical]=X_transformed[:, categorical].astype(str)\n",
    "    return X_transformed,categorical\n",
    "\n",
    "    \n",
    "def CatBoostSearch(X,y,metric=\"AUC\"): \n",
    "    param_grid_cat = {\n",
    "    'iterations': [100,200],           # Number of boosting iterations (trees)\n",
    "    'learning_rate': [0.001],     # Step size shrinkage\n",
    "    'depth': [4],                      # Depth of each tree\n",
    "    'l2_leaf_reg': [1],                # L2 regularization\n",
    "    'border_count': [32],           # Number of splits for numerical features                  \n",
    "    'auto_class_weights':['Balanced'],           # Class weight scaling (for imbalanced data)\n",
    "    'grow_policy': ['SymmetricTree'],  \n",
    "    'colsample_bylevel': [1.0],         # Subsample ratio of columns for each split level\n",
    "    'min_data_in_leaf': [1],         # Minimum samples in leaf\n",
    "    'max_leaves': [31]\n",
    "    }\n",
    "    X_train,cat_features=MergeNumCat(X)\n",
    "    \n",
    "    train_pool = Pool(data=X_train, label=y, cat_features=cat_features)\n",
    "    \n",
    "    model = CatBoostClassifier(verbose=1,)\n",
    "    \n",
    "    grid_result=model.grid_search(param_grid_cat, X=train_pool, y=None,cv=5,refit=True)\n",
    "    \n",
    "    with open(f\"savedModels/CatBoostClassifier.json\", \"w\") as f:\n",
    "        json.dump(grid_result, f, indent=4)\n",
    "    return  grid_result\n",
    "\n",
    "# def GridCatBoostSearch(params,df_X,y,metric=\"AUC\",n_splits=5):\n",
    "#     BestModel=None \n",
    "#     BestAucScore=float(\"-inf\")\n",
    "#     BestIteration=None\n",
    "#     X,cat_features=MergeNumCat(df_X)\n",
    "  \n",
    "#     cat_features=list(cat_features)\n",
    "#     X[:, cat_features]=X[:, cat_features].astype(str)\n",
    "\n",
    "#     for param in  list(ParameterGrid(params)): \n",
    "        \n",
    "#         Model=CatBoostClassifier(eval_metric=metric,early_stopping_rounds =15,random_state=42,**param)\n",
    "#         SplitMatrix=kfold_split_np(X,y, n_splits)\n",
    "#         aucVector=np.zeros(n_splits)\n",
    "#         NumberIterations=np.zeros(n_splits) \n",
    "#         counter=0\n",
    "#         for split in SplitMatrix:\n",
    "#             X_train, X_test, y_train, y_test=split[0],split[1],split[2],split[3]\n",
    "#             train_pool=Pool(data=X_train,label=y_train,cat_features=cat_features)\n",
    "#             validation_pool = Pool(data=X_test,label=y_test,cat_features=cat_features)            \n",
    "#             Model.fit(train_pool,eval_set=validation_pool)\n",
    "#             y_proba = Model.predict_proba(X_test)[:, 1]\n",
    "#             auc = roc_auc_score(y_test, y_proba)\n",
    "#             aucVector[counter]=auc \n",
    "#             NumberIterations[counter]=Model.get_best_iteration()\n",
    "#             counter+=1\n",
    "#         meanAuc=np.mean(aucVector)\n",
    "#         meanBestIteration=np.mean(NumberIterations)\n",
    "#         if BestAucScore<meanAuc:\n",
    "#             BestAucScore=meanAuc\n",
    "#             BestModel=param\n",
    "#             BestIteration=meanBestIteration\n",
    "#     BestModelo[\"iterations\"]=int(round(BestIterationt))\n",
    "#     with open(f\"savedModels/CatBoostClassifier.json\", \"w\") as f:\n",
    "#         json.dump(BestModel, f, indent=4)\n",
    "#     return BestAucScore,BestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef693b-bc90-43c7-b46d-f1b028fe02f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiantSearch(): \n",
    "    X,y=KCrossData()\n",
    "    CatBoostSearch(X,y)\n",
    "    XgboostSearch(GetScalePosWeight(y),X,y)\n",
    "    LightgbmSearch(X,y)\n",
    "    AdaSearch(X,y) \n",
    "    print(\"Well done\") \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4a52ea0e-4ceb-4851-a41c-aeecda5a4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT SURE WHETER IT WORKS\n",
    "\n",
    "def ImportanceGetter(pipeline): \n",
    "    model = pipeline.named_steps[\"model\"]\n",
    "    importances = model.feature_importances_\n",
    "    preprocessor = pipeline.named_steps[\"preprocessing\"]\n",
    "    feature_names = preprocessor.get_feature_names_out()\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": feature_names,\n",
    "    \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "    return  feature_importance_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
