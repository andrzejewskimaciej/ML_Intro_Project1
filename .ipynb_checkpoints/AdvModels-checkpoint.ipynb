{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1f9b42-c61b-4e30-8443-1bd126d2351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run ColumnTransformers.ipynb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebb8187-2fee-4cc0-ad8a-5826ab8b2789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XgboostSearch(scale_pos_weight,X,y): \n",
    "    param_grid_xgb = {\n",
    "        'model__n_estimators': [100, 300,500],  \n",
    "        'model__learning_rate': [0.001,0.01],  \n",
    "        'model__max_depth': [2,3,4], \n",
    "        'model__subsample': [0.7, 1.0],  \n",
    "        'model__colsample_bytree': [0.6, 1.0],  \n",
    "        'model__gamma': [0, 0.1, 0.5 ], \n",
    "        'model__reg_lambda': [0,1, 4], \n",
    "        'model__scale_pos_weight':[scale_pos_weight], # weight = 1 if negstive else wegiht=scale_pos_weight\n",
    "        }  \n",
    "    grid_search_XgBoost = GridSearchgGradient(XGBClassifier(eval_metric='auc',random_state=42),param_grid_xgb,X,y)\n",
    "    return grid_search_XgBoost\n",
    "\n",
    "\n",
    "def LightgbmSearch(ModelPipelineX,y):\n",
    "    param_grid_lgb = {\n",
    "        'model__n_estimators': [100, 300, 500],\n",
    "        'model__learning_rate': [0.001, 0.01],\n",
    "        'model__max_depth': [2, 3, 4],\n",
    "        'model__subsample': [0.7, 1.0],\n",
    "        'model__colsample_bytree': [0.6, 1.0],\n",
    "        'model__min_split_gain': [0, 0.1, 0.5],   # similar to gamma from XGB\n",
    "        'model__reg_lambda': [0, 1, 4], \n",
    "        'is_unbalance':[True] , \n",
    "        'model__early_stopping_rounds': [15]}\n",
    "\n",
    "    grid_search_LightGBM = GridSearchgGradient(LGBMClassifier(eval_metric='auc',random_state=42),param_grid_lgb,X,y)\n",
    "    return grid_search_LightGBM\n",
    "\n",
    "def AdaSearch(ModelPipeline) : \n",
    "    param_grid_ada = {\n",
    "    'model__n_estimators': [100,200], \n",
    "    'model__learning_rate': [0.6,1,2],  # before amount of say\n",
    "    'model__estimator': [DecisionTreeClassifier(max_depth=2), \n",
    "                          DecisionTreeClassifier(max_depth=3),\n",
    "                          DecisionTreeClassifier(max_depth=5)]\n",
    "    }\n",
    "    grid_search_ada = GridSearchCV(ModelPipeline, param_grid_ada, cv=5, scoring='roc_auc', n_jobs=3)\n",
    "    return grid_search_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c811393-fb6a-4c7c-91c6-ba223d703956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_split(X, y, n_splits=5, shuffle=True, random_state=42): \n",
    "    n_splits=5 \n",
    "    n_samples = len(X)\n",
    "    indices = np.arange(n_samples)\n",
    "    \n",
    "    if shuffle:\n",
    "      rng = np.random.default_rng(seed=random_state) \n",
    "      rng.shuffle(indices)\n",
    "    \n",
    "    fold_sizes = np.full(n_splits, n_samples // n_splits, dtype=int)  # creating kinda equal split\n",
    "    fold_sizes[:n_samples % n_splits] +=1  # distribute the remainder among the first folds\n",
    "    current = 0\n",
    "    fold_sizes=np.cumsum(fold_sizes)\n",
    "    ResultMatrix=[]\n",
    "    for fold_size in fold_sizes: \n",
    "        X_test=X.iloc[indices[current:fold_size]]\n",
    "        y_test=y.iloc[indices[current:fold_size]]\n",
    "        X_train=X.iloc[np.concatenate((indices[:current],indices[fold_size:]))]\n",
    "        y_train=y.iloc[np.concatenate((indices[:current],indices[fold_size:]))]\n",
    "        ResultMatrix.append((X_train, X_test, y_train, y_test))\n",
    "        current=fold_size\n",
    "    return ResultMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d814dc6a-e78e-4610-afbc-0bd625b22be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearchgGradient(GradientModel,params,X,y,model__early_stopping=15, n_splits=5):\n",
    "    BestModelParams=None \n",
    "    BestAucScore=float(\"-inf\")\n",
    "    for param in  list(ParameterGrid(params)): \n",
    "        Model=PipelineModel(GradientModel) \n",
    "        Model.set_params(**param)\n",
    "        SplitMatrix=kfold_split(X,y, n_splits)\n",
    "        aucVector=np.zeros(n_splits)\n",
    "        counter=0\n",
    "        for split in SplitMatrix:\n",
    "            X_train, X_test, y_train, y_test=split[0],split[1],split[2],split[3]\n",
    "            print(\"123\")\n",
    "            Model.fit(X_train,y_train,model__eval_set=[(X_test, y_test)],\n",
    "                model__early_stopping_rounds=model__early_stopping) \n",
    "            y_proba = Model.predict_proba(X_test)[:, 1]\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "            aucVector[counter]=auc \n",
    "            counter+=1\n",
    "            \n",
    "        if BestAucScore<np.mean(aucVector):\n",
    "            BestAucScore=auc\n",
    "            BestModel=param\n",
    "    return BestAucScore,BestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e93d2ec3-2641-48e8-833b-8b4cf75c4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pipeline.fit does not accept the early_stopping_rounds parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m X,y=KCrossData()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mXgboostSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetScalePosWeight\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mXgboostSearch\u001b[39m\u001b[34m(scale_pos_weight, X, y)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mXgboostSearch\u001b[39m(scale_pos_weight,X,y): \n\u001b[32m      2\u001b[39m     param_grid_xgb = {\n\u001b[32m      3\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel__n_estimators\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m100\u001b[39m, \u001b[32m300\u001b[39m,\u001b[32m500\u001b[39m],  \n\u001b[32m      4\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel__learning_rate\u001b[39m\u001b[33m'\u001b[39m: [\u001b[32m0.001\u001b[39m,\u001b[32m0.01\u001b[39m],  \n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mmodel__scale_pos_weight\u001b[39m\u001b[33m'\u001b[39m:[scale_pos_weight], \u001b[38;5;66;03m# weight = 1 if negstive else wegiht=scale_pos_weight\u001b[39;00m\n\u001b[32m     11\u001b[39m         }  \n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     grid_search_XgBoost = \u001b[43mGridSearchgGradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXGBClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauc\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mparam_grid_xgb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m grid_search_XgBoost\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mGridSearchgGradient\u001b[39m\u001b[34m(GradientModel, params, X, y, model__early_stopping, n_splits)\u001b[39m\n\u001b[32m     11\u001b[39m X_train, X_test, y_train, y_test=split[\u001b[32m0\u001b[39m],split[\u001b[32m1\u001b[39m],split[\u001b[32m2\u001b[39m],split[\u001b[32m3\u001b[39m]\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m123\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel__eval_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel__early_stopping\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     15\u001b[39m y_proba = Model.predict_proba(X_test)[:, \u001b[32m1\u001b[39m]\n\u001b[32m     16\u001b[39m auc = roc_auc_score(y_test, y_proba)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Programming\\PycharmProjects\\ML_Intro-Projects\\venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Programming\\PycharmProjects\\ML_Intro-Projects\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:653\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform_input \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    647\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    648\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe `transform_input` parameter can only be set if metadata \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    649\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrouting is enabled. You can enable metadata routing using \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    650\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`sklearn.set_config(enable_metadata_routing=True)`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    651\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m653\u001b[39m routed_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_method_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprops\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    654\u001b[39m Xt = \u001b[38;5;28mself\u001b[39m._fit(X, y, routed_params, raw_params=params)\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[33m\"\u001b[39m\u001b[33mPipeline\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m._log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.steps) - \u001b[32m1\u001b[39m)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Programming\\PycharmProjects\\ML_Intro-Projects\\venv\\Lib\\site-packages\\sklearn\\pipeline.py:459\u001b[39m, in \u001b[36mPipeline._check_method_params\u001b[39m\u001b[34m(self, method, props, **kwargs)\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m pname, pval \u001b[38;5;129;01min\u001b[39;00m props.items():\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pname:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    460\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPipeline.fit does not accept the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m parameter. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYou can pass parameters to specific steps of your \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    462\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpipeline using the stepname__parameter format, e.g. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    463\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`Pipeline.fit(X, y, logisticregression__sample_weight\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    464\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m=sample_weight)`.\u001b[39m\u001b[33m\"\u001b[39m.format(pname)\n\u001b[32m    465\u001b[39m         )\n\u001b[32m    466\u001b[39m     step, param = pname.split(\u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    467\u001b[39m     fit_params_steps[step][\u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m][param] = pval\n",
      "\u001b[31mValueError\u001b[39m: Pipeline.fit does not accept the early_stopping_rounds parameter. You can pass parameters to specific steps of your pipeline using the stepname__parameter format, e.g. `Pipeline.fit(X, y, logisticregression__sample_weight=sample_weight)`."
     ]
    }
   ],
   "source": [
    "X,y=KCrossData()\n",
    "XgboostSearch(GetScalePosWeight(y),X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4955f9f7-8171-4db2-a9d5-383419f0b3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
